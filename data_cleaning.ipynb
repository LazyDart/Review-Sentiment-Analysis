{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 10:05:30.881489: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-11 10:05:31.636991: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Data Wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Eventual Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Text Processing\n",
    "import emoji\n",
    "import re\n",
    "import string\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "\n",
    "# Language Detection\n",
    "from transformers import pipeline\n",
    "\n",
    "# Spell Checking\n",
    "import enchant\n",
    "from autocorrect import Speller\n",
    "from enchant.checker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ready scraped data\n",
    "df = pd.read_csv(\"./data/merged_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: review_text, dtype: object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[(df[\"offer_ref\"].isin([119352308, 126556830, 97049775,]) \n",
    "        & df[\"entry_id\"].isin([16919388, 16453304, 16368349,])),\n",
    "        \"review_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete reviews written in cyrillic with a mix of polish characters (Those cases are not handled by cyryllic fix later on)\n",
    "df = df[~(df[\"offer_ref\"].isin([119352308, 126556830, 97049775,]) \n",
    "          & df[\"entry_id\"].isin([16919388, 16453304, 16368349,]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_langs = pd.read_csv(\"./data/foreign_languages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: review_text, dtype: object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df[\"entry_id\"].isin(other_langs[\"entry_id\"]) \n",
    "       & df[\"offer_ref\"].isin(other_langs[\"offer_ref\"]), \n",
    "       \"review_text\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete reviews written in other languages\n",
    "df = df.loc[~(df[\"entry_id\"].isin(other_langs[\"entry_id\"]) \n",
    "              & df[\"offer_ref\"].isin(other_langs[\"offer_ref\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"review_text\" nan are to be deleted. Other can stay.\n",
    "df = df.dropna(subset=[\"review_text\"])\n",
    "\n",
    "# Remove invalid entries\n",
    "df = df.loc[df[\"entry_date\"] != \"entry_date\"]\n",
    "\n",
    "# Remove duplicate entries\n",
    "df = df.drop_duplicates([\"offer_ref\", \"entry_id\", \"review_text\"])\n",
    "\n",
    "# fix data types\n",
    "df[\"entry_date\"] = pd.to_datetime(df[\"entry_date\"])\n",
    "df[\"purchase_date\"] = pd.to_datetime(df[\"purchase_date\"])\n",
    "df[\"entry_id\"] = df[\"entry_id\"].astype(int)\n",
    "df[\"offer_ref\"] = df[\"offer_ref\"].astype(int)\n",
    "df[\"score\"] = df[\"score\"].astype(float)\n",
    "\n",
    "# Get Sentiment Cases based on score\n",
    "df[\"sentiment\"] = df[\"score\"].apply(lambda x: \"Positive\" if x >= 4 else \"Negative\" if x <= 2 else \"Neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only text and sentiment\n",
    "to_clean = df[[\"review_text\", \"sentiment\"]].drop_duplicates(\"review_text\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4877, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1927 reviews containing special unicode characters\n",
    "to_clean[to_clean[\"review_text\"].str.contains(\"[\\t\\r\\n\\v\\f\\ufeff]\")].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of newlines and other whitespace chars\n",
    "to_clean[\"review_text\"] = to_clean[\"review_text\"].replace(\"[\\t\\r\\n\\v\\f\\ufeff]\", \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [review_text, sentiment]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove whitespaces created by previous step\n",
    "to_clean[\"review_text\"] = to_clean[\"review_text\"].replace(\" +\", \" \", regex=True)\n",
    "to_clean[to_clean[\"review_text\"].str.contains(\"  \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove cyrillic characters. Sometimes there are reviews half written in cyrillic and half in polish, so I will keep polish characters.\n",
    "cyrylic_regex = f\"[ {string.punctuation}0-9]*[\\u0400-\\u04FF]+[ {string.punctuation}0-9]*[\\u0400-\\u04FF]+[ {string.punctuation}0-9]*\"\n",
    "to_clean[\"review_text\"] = to_clean[\"review_text\"].transform(lambda x: re.sub(cyrylic_regex, \"\", x)).replace(\"\", np.nan)\n",
    "to_clean = to_clean.dropna(subset=[\"review_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ckpt = \"papluca/xlm-roberta-base-language-detection\"\n",
    "# pipe = pipeline(\"text-classification\", model=model_ckpt)\n",
    "# language_classification = pipe(to_clean[\"review_text\"].transform(lambda x: x[:100]).to_list(), top_k=1, truncation=True)\n",
    "# to_clean = pd.concat([to_clean.reset_index(), pd.DataFrame(np.array(language_classification).reshape(-1).tolist())], axis=1).set_index(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lot of Wrong predictions. I will fix them manually\n",
    "# to_clean[(to_clean[\"label\"] != \"pl\")].iloc[2400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark all emojis in data. All_emoji will be excluded entirely, and has_emoji will remove only emojis.\n",
    "to_clean[\"has_emoji\"] = to_clean[\"review_text\"].transform(lambda x: np.any([emoji.is_emoji(c) for c in x]))\n",
    "to_clean[\"all_emoji\"] = to_clean[\"review_text\"].transform(lambda x: np.all([emoji.is_emoji(c) for c in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "# 460 Reviews containing emojis\n",
    "print(len(to_clean[to_clean[\"has_emoji\"]]))\n",
    "# 42 reviews containing only emojis\n",
    "print(len(to_clean[to_clean[\"all_emoji\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>has_emoji</th>\n",
       "      <th>all_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>😑😑</td>\n",
       "      <td>Negative</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>😐</td>\n",
       "      <td>Negative</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9421</th>\n",
       "      <td>👍</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10053</th>\n",
       "      <td>👍👍👍👍👍</td>\n",
       "      <td>Positive</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10958</th>\n",
       "      <td>🤗</td>\n",
       "      <td>Positive</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11179</th>\n",
       "      <td>🥳🎉</td>\n",
       "      <td>Positive</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13728</th>\n",
       "      <td>👍🏻👍🏻</td>\n",
       "      <td>Negative</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14711</th>\n",
       "      <td>👍👍</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24204</th>\n",
       "      <td>🙂</td>\n",
       "      <td>Positive</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25324</th>\n",
       "      <td>😫</td>\n",
       "      <td>Negative</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      review_text sentiment  has_emoji  all_emoji\n",
       "0              😑😑  Negative       True       True\n",
       "1091            😐  Negative       True       True\n",
       "9421            👍   Neutral       True       True\n",
       "10053       👍👍👍👍👍  Positive       True       True\n",
       "10958           🤗  Positive       True       True\n",
       "11179          🥳🎉  Positive       True       True\n",
       "13728        👍🏻👍🏻  Negative       True       True\n",
       "14711          👍👍   Neutral       True       True\n",
       "24204           🙂  Positive       True       True\n",
       "25324           😫  Negative       True       True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can sort of guess the sentiment of the review based on the emojis. But it's not consistent.\n",
    "to_clean[to_clean[\"all_emoji\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all reviews containing single repeating characters that are non-emoji\n",
    "to_clean = to_clean.loc[~(to_clean[\"review_text\"].transform(lambda x: len(set(x)) == 1) & ~to_clean[\"all_emoji\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5675/585859017.py:6: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  & ~to_clean[\"review_text\"].str.lower().str.contains(\"(ok|[0-9]|:[\\)\\(]|;[\\)\\()\\/]|\\*|bdb|kk|x)\"))\n"
     ]
    }
   ],
   "source": [
    "# remove reviews with only 3 unique characters that are not meaninful. Decided by hand.\n",
    "# Meaningfull list contains different versions of emoticons and ok and kk and bdb and x... and 5/5 etc.\n",
    "to_clean = to_clean[\n",
    "    ~(to_clean[\"review_text\"].transform(lambda x: len(set(x)) < 3) \n",
    "    & ~to_clean[\"has_emoji\"] \n",
    "    & ~to_clean[\"review_text\"].str.lower().str.contains(\"(ok|[0-9]|:[\\)\\(]|;[\\)\\()\\/]|\\*|bdb|kk|x)\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146       Odradzam ze zwględu na funkcjonalność. Bardzo ...\n",
       "428       Wykonanie bardo dobre. Co do baterii, tu trzeb...\n",
       "804        Konto Nie polecam Ocena: 1/5 Wystawiono 2 mie...\n",
       "1085      Jedyna zaleta to wysokość. . Odkurzanie niedok...\n",
       "1798      [...] A teraz krótko o baterii: przestrzegam i...\n",
       "                                ...                        \n",
       "140820    Omron 3 (zakupiony w maju 2013 r.; w aptece) b...\n",
       "142821    Po pomalowaniu (na bardzo jasnej ścianie) i wy...\n",
       "159334    Kocioł użytkuję od 2 lat. Trzeba dobrze wyregu...\n",
       "166906    Zapach podoba się każdemu :) Osobiscie używam ...\n",
       "172943    Telefon posiadałem 2 miesiące i z przykrością ...\n",
       "Name: review_text, Length: 73, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(to_clean.loc[\n",
    "        to_clean[\"review_text\"].transform(lambda x: len(re.findall(r\"\\bok\\b.? [0-9]+\", x))) > 1,\n",
    "        \"review_text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function discriminates between ok meaning good and ok meaning circa \n",
    "# which is a short form of około in polish language.\n",
    "# Ok has it's own fix because it is very common in reviews and gives positive sentiment.\n",
    "def fix_ok_in_string(text):\n",
    "    circa_match = re.search(r\"\\bok\\b.? [0-9]+\", text.lower())\n",
    "    if circa_match is not None:\n",
    "        circa_match = circa_match.span()\n",
    "        return re.sub(r\"\\b(ok|Ok|OK|oK)\\b\\.?( [^0-9])\", r\"Ok\\g<2>\", text)\n",
    "    else:\n",
    "        return re.sub(r\"\\b(ok|Ok|OK|oK)\\b\", \"Ok\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oczyszczacz Ok kupiony w RTVEuroAGD za 299pln miał być \"okazją stulecia\". Na stronie polskiego importera widniała cena ponad 1000pln !!!!! Po zakupie wyszło to: - lampa UV ma żywotność 600h, co kwalifikuje ją do wymiany co ok. 2 m-ce; koszt ok. 40zł plus transport (swoją drogą ciężko ocenić jej działanie; instrukcja zabrania też używanie oczyszczacza bez lub z niedziałającą lampą) - filtr ściemniał i pomimo czyszczenia już się praktycznie nie nadaje do użytku po 13 miesiącach - ma dziwny zapach, więc pewnie powinien być wymieniony nieco wcześniej; koszt ok. 70pln plus pewnie jakiś transport - denerwowało mnie również, że oczyszczacz stojąc na podłodze i w trybie nadmuchu \"2\" lub \"3\" ODGANIA OD SIEBIE brud zamiast go wciągać !!!! Sprzedałem niedawno na licytacji Allegro za 77zł (słabe zainteresowanie). Rok korzystania za ok. 280pln. Może opłacałoby się dalej inwestować w filtry i lampy UV, ale doszedłem do wniosku, że ten sprzęt jest po prostu słaby. Szybko nie zdecyduję się na nic Ok podobnego.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = (to_clean.loc[\n",
    "        to_clean[\"review_text\"].transform(lambda x: len(re.findall(r\"\\bok\\b.? [0-9]+\", x))) > 2,\n",
    "        \"review_text\"]\n",
    ").iloc[0]\n",
    "\n",
    "test_text = test_text[:12] + \"ok \" + test_text[12:-10] + \"ok \" + test_text[-10:]\n",
    "\n",
    "fix_ok_in_string(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark Ok's in data.\n",
    "to_clean[\"has_ok\"] = to_clean[\"review_text\"].str.lower().str.contains(\"\\Wok\\W\")\n",
    "# to_clean[to_clean[\"has_ok\"]] = to_clean[to_clean[\"has_ok\"]].replace(r\"\\bok\\b\", \"Ok\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ok. lalala ok. 100kg Ok!'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_ok_in_string(\"ok. lalala ok. 100kg ok!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix all ok's in data.\n",
    "to_clean[\"review_text\"] = to_clean[\"review_text\"].transform(fix_ok_in_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not Currently Used\n",
    "# Remove all numbers from data.\n",
    "# to_clean[\"review_text\"] = to_clean[\"review_text\"].transform(lambda x: re.sub(\"[0-9]+\", \"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Largest default polish language model\n",
    "nlp = spacy.load(\"pl_core_news_lg\")\n",
    "\n",
    "# Autocorrect speller\n",
    "spell = Speller(\"pl\", only_replacements=True)\n",
    "\n",
    "# Enchant spellcheckers\n",
    "chkr = SpellChecker(\"pl_Pl\") \n",
    "d_typo = enchant.Dict(\"pl_PL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During cleaning new duplicate reviews were created.\n",
    "to_clean = to_clean.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5675/486839767.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  to_clean[\"review_text\"] = to_clean[\"review_text\"].transform(lambda x: nlp(x))\n"
     ]
    }
   ],
   "source": [
    "to_clean[\"review_text\"] = to_clean[\"review_text\"].transform(lambda x: nlp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5675/3802480203.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  to_clean[\"Typo Data\"] = (\n"
     ]
    }
   ],
   "source": [
    "to_clean[\"Typo Data\"] = (\n",
    "    to_clean[\"review_text\"].transform(lambda x: \n",
    "        [[token.text, \n",
    "          d_typo.suggest(token.text), \n",
    "          i, \n",
    "          str(x).find(token.text), \n",
    "          len(token.text),\n",
    "          ] \n",
    "        for i, token in enumerate(x) \n",
    "            if not token.is_punct\n",
    "               and not token.is_space\n",
    "               and not token.like_num\n",
    "               and (not d_typo.check(token.text))\n",
    "               and (not d_typo.check(token.text.lower())) \n",
    "               and (not d_typo.check(token.text.capitalize())) \n",
    "               and (emoji.is_emoji(token.text) == False)])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_clean.to_pickle(\"./data/typos_annotated.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will run 3 mins\n",
    "\n",
    "# Get subset of data.\n",
    "# checker_test = to_clean[\"review_text\"].iloc[:2000].transform(lambda x: nlp(x))\n",
    "\n",
    "# 42% of first 2000 reviews contain typos\n",
    "# pyenchany seems good at detecting typos however it seems bad at fixing typos So we'll have to be careful with it.\n",
    "# checker_test[checker_test.transform(lambda x: all([d_typo.check(token.text.lower()) for token in x if not token.is_punct])) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes 44 mins.\n",
    "## Fix all typos. Write 2 csv files with and without type fixing. Used for comparison in another script.\n",
    "# to_clean[\"review_text\"].transform(lambda x: [d_typo.suggest(token.text)[0] if (not token.is_punct) and (not d_typo.check(token.text)) and (chkr.suggest(token.text))  else token for token in x]).to_csv(\"Testing Typo Checking.csv\")\n",
    "# to_clean[\"review_text\"].to_csv(\"Comparison to a Typo fix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've forgotten about sentiment.\n",
    "# to_clean[\"sentiment\"].to_csv(\"sentiment provision.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
